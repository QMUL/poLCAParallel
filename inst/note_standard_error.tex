\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{graphicx}

\usepackage[round]{natbib}

\newcommand{\T}{^\textup{T}}

\begin{filecontents*}[overwrite]{bib.bib}
@article{JSSv042i10,
 title={{poLCA}: An {R} Package for Polytomous Variable Latent Class Analysis},
 volume={42},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v042i10},
 doi={10.18637/jss.v042.i10},
 abstract={&amp;lt;b&amp;gt;poLCA&amp;lt;/b&amp;gt; is a software package for the estimation of latent class and latent class regression models for polytomous outcome variables, implemented in the R statistical computing environment. Both models can be called using a single simple command line. The basic latent class model is a finite mixture model in which the component distributions are assumed to be multi-way cross-classification tables with all variables mutually independent. The latent class regression model further enables the researcher to estimate the effects of covariates on predicting latent class membership. &amp;lt;b&amp;gt;poLCA&amp;lt;/b&amp;gt; uses expectation-maximization and Newton-Raphson algorithms to find maximum likelihood estimates of the model parameters.},
 number={10},
 journal={Journal of Statistical Software},
 author={Linzer, Drew A. and Lewis, Jeffrey B.},
 year={2011},
 pages={1â€“29}
}

@article{doi:10.1080/01621459.1997.10473658,
author = { Karen   Bandeen-roche  and  Diana L.   Miglioretti  and  Scott L.   Zeger  and  Paul J.   Rathouz },
title = {Latent Variable Regression for Multiple Discrete Outcomes},
journal = {Journal of the American Statistical Association},
volume = {92},
number = {440},
pages = {1375-1386},
year  = {1997},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.1997.10473658},
URL = { https://doi.org/10.1080/01621459.1997.10473658},
eprint = { https://doi.org/10.1080/01621459.1997.10473658}
}

@article{10.2307/2345828,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2345828},
 abstract = {A procedure is derived for extracting the observed information matrix when the EM algorithm is used to find maximum likelihood estimates in incomplete data problems. The technique requires computation of a complete-data gradient vector or second derivative matrix, but not those associated with the incomplete data likelihood. In addition, a method useful in speeding up the convergence of the EM algorithm is developed. Two examples are presented.},
 author = {Thomas A. Louis},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {2},
 pages = {226--233},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Finding the Observed Information Matrix when Using the {EM} Algorithm},
 volume = {44},
 year = {1982}
}

@article{efron1979bootstrap,
  author = {Efron, B.},
  journal = {The Annals of Statistics},
  number = {1},
  pages = {1-26},
  title = {Bootstrap Methods: Another Look at the {J}ackknife},
  volume = {7},
  year = {1979}
}

\end{filecontents*}

\title{On the Standard Error in \texttt{poLCA}}
\author{Sherman Lo}
\date{}

\begin{document}
\maketitle

This note describes the current implementation of the standard error in the \texttt{R} package \texttt{poLCA}. The mathematics is explained in \cite{JSSv042i10}. The observed information matrix is constructed, using the data and estimated parameters, which is then used to solve a linear equation. Computationally, there are flaws in the implementation especially when the observed information matrix is, or close to, being singular.

The mathematical problem shall be explained here along with an explanation of the \texttt{poLCA} implementation of it. The problems are discussed here along with possible solutions and further discussions.

The notation tries to be as similar as possible to \cite{JSSv042i10}.

\section{Standard Error}

The latent class model is discussed here, but can be extended to the latent class regression models.

Suppose there are $N$ data points, each with log likelihood
\begin{equation}
    \ln L_i = \sum_{i=1}^N \ln \sum_{r=1}^R p_r \prod_{j=1}^J \prod_{k=1}^{K_j} \pi_{j,r,k}^{Y_{i,j,k}}
\end{equation}
where there are $R$ clusters, $J$ categories, $K_1, K_2,\ldots,K_J$ are the number of responses for each category, $p_r$ is the prior probability, $\pi_{j,r,k}$ is the outcome probability and $Y_{i,j,k}$ is the observed response for $i=1,2,\ldots,N$, $j=1=2,\ldots,J$, $r=1,2,\ldots,R$ and for a given $j$, $k=1,2,\ldots,K_j$.

In addition, the parameters are re-parameterised such that
\begin{equation}
    p_1 = \dfrac{1}{\sum_{r'=2}^R \exp(\omega_{r'})}
\end{equation}
\begin{equation}
    p_r = \dfrac{\exp(\omega_{r'})}{\sum_{r'=2}^R \exp(\omega_{r'})}
\end{equation}
\begin{equation}
    \pi_{j,r,1} = \dfrac{1}{\sum_{k'=2}^{K_j} \exp(\phi_{j,r,k'})}
\end{equation}
\begin{equation}
    \pi_{j,r,k} = \dfrac{\exp(\phi_{j,r,k})}{\sum_{k'=2}^{K_j} \exp(\phi_{j,r,k'})}
    \ .
\end{equation}

The score vector $\mathbf{s}_i$ is
\begin{equation}
    \mathbf{s}_i = \mathbf{s}_i(Y_i, \Psi) = \nabla_\Psi  \ln L_i
\end{equation}
where
\begin{equation}
    \Psi =
    \begin{pmatrix}
        \omega_2 \\
        \vdots \\
        \omega_R \\
        \phi_{1,1,2} \\
        \vdots \\
        \phi_{1,1,K_1} \\
        \phi_{2,1,1} \\
        \vdots \\
        \phi_{J,1,K_J} \\
        \phi_{1,2,2} \\
        \vdots \\
        \phi_{J, R, K_J}
    \end{pmatrix}
\end{equation}
is a vector of all parameters. The scores are calculated to be
\begin{equation}
    \mathbf{s}_i
    =
    \begin{pmatrix}
        \dfrac{\partial \ln L_i}{\partial \omega_2} \\ \vspace{1em}
        \vdots \\ \vspace{1em}
        \dfrac{\partial \ln L_i}{\partial \omega_R} \\ \vspace{1em}
        \dfrac{\partial \ln L_i}{\partial \phi_{1,1,2}} \\ \vspace{1em}
        \vdots \\
        \dfrac{\partial \ln L_i}{\partial \phi_{J,R,K_J}}
    \end{pmatrix}
    =
    \begin{pmatrix}
        \theta_{i2} - p_{2} \\
        \vdots \\
        \theta_{iR} - p_{R} \\
        \theta_{ir}(Y_{i,1,2} - \pi_{1,1,2}) \\
        \vdots \\
        \theta_{ir}(Y_{i,J,K_J} - \pi_{J,R,K_J}) \\
    \end{pmatrix}
\end{equation}
where $\theta_{i,r}$ are the posterior probabilities
\begin{equation}
    \theta_{i,r} = \dfrac{
        p_r \prod_{j=1}^J \prod_{k=1}^{K_j} \pi_{j,r,k}^{Y_{i,j,k}}
    } {
        \sum_{r'=1}^R p_{r'} \prod_{j=1}^J \prod_{k=1}^{K_j} \pi_{j,r',k}^{Y_{i,j,k}}
    }
    \ .
\end{equation}

The observed information matrix $\mathbf{I}$ is constructed using the collection of the score vectors
\begin{equation}
    \mathbf{I} = \sum_{i=1}^N \mathbf{s}_i \mathbf{s}_i\T
\end{equation}
which can be written in matrix form
\begin{equation}
    \mathbf{I} = \mathbf{S}\T \mathbf{S}
\end{equation}
where $\mathbf{S}$ is the design matrix of score vectors
\begin{equation}
    \mathbf{S} =
    \begin{pmatrix}
      \mathbf{s}_1\T \\
      \mathbf{s}_2\T \\
      \vdots \\
      \mathbf{s}_N\T
    \end{pmatrix}
    \ .
\end{equation}

The delta method can be written as
\begin{equation}
    \Sigma = \mathbf{J}\T \mathbf{I}^{-1} \mathbf{J}
\end{equation}
where $\mathbf{J}$ is the Jacobian matrix and has the form
\begin{equation}
    \mathbf{J} =
    \begin{pmatrix}
        \mathbf{J}_\omega & 0 & 0 & \ldots & 0\\
        0 & \mathbf{J}_{\phi_{1,1}} & 0 & \cdots & 0\\
        0 & 0 & \mathbf{J}_{\phi_{2,1}} & \cdots & 0 \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & \cdots & \mathbf{J}_{\phi_{J,R}}
    \end{pmatrix}
\end{equation}
where
\begin{equation}
    \mathbf{J}_\omega =
    \begin{pmatrix}
        -p_1 p_2 & p_2 (1-p_2) & -p_3 p_2 & \cdots & -p_R p_2 \\
        -p_1 p_3 & -p_2 p_3 & p_3 (1-p_3) & \cdots & -p_R p_3\\
        \vdots & \vdots  & \vdots & \ddots & \vdots \\
        -p_1 p_R & -p_2 p_R & -p_3 p_R & \cdots & p_R (1-p_R)
    \end{pmatrix}
\end{equation}
and
\begin{equation}
    \mathbf{J}_{\phi_{j,r}} =
    \begin{pmatrix}
        -\pi_{j,r,1} \pi_{j,r,2} & \pi_{j,r,2} (1-\pi_{j,r,2}) & -\pi_{j,r,3} \pi_{j,r,2} & \cdots & -\pi_{j,r,K_j} \pi_{j,r,2} \\
        -\pi_{j,r,1} \pi_{j,r,3}& -\pi_{j,r,2} \pi_{j,r,3} & \pi_{j,r,3} (1-\pi_{j,r,3}) & \cdots & -\pi_{j,r,K_j} \pi_{j,r,3}\\
        \vdots & \vdots  & \vdots & \ddots & \vdots \\
        -\pi_{j,r,1} \pi_{j,r,K_j} & -\pi_{j,r,2} \pi_{j,r,K_j} & -\pi_{j,r,3} \pi_{j,r,K_j} & \cdots & \pi_{j,r,K_j} (1-\pi_{j,r,K_j})
    \end{pmatrix}
    \ .
\end{equation}

The standard errors of interest are the diagonal elements of $\Sigma = \mathbf{J}\T \mathbf{I}^{-1} \mathbf{J}$.

\section{Implementation}

The package \texttt{poLCA} uses the pseudo-inverse to calculate $\mathbf{I}^{-1}$. Calculating the inverse of a matrix is highly discouraged as it is known to be inaccurate and slow. The current implementation also does the full matrix multiplication $\mathbf{J}\T \mathbf{I}^{-1} \mathbf{J}$, however, this could be wasteful because only the diagonal elements are needed.

It should be faster to do a Cholesky decomposition of
\begin{equation}
    \mathbf{I}=\mathbf{L}\mathbf{L}\T
\end{equation}
where $\mathbf{L}$ is a lower triangular matrix. Then the linear equation becomes
\begin{equation}
    \Sigma = (\mathbf{L}^{-1}\mathbf{J})\T \mathbf{L}^{-1} \mathbf{J} \ .
\end{equation}
The standard errors can be obtained by taking the column sum of squares of $\mathbf{L}^{-1} \mathbf{J}$ which can be solved using a lower triangular solver.

However, it is not uncommon for $\mathbf{I}^{-1}$ to be (or close to) singular. This can happen when the EM algorithm overfits especially when the posteriors $\theta_{ir}$ and estimated probabilities $\pi_{j,r,k}$ take values of (or close to) zero or one. This can cause $\theta_{ir}(Y_{i,r,k}-\pi_{j,r,k})=0$ for all $i$ and make at least one row and column of $\mathbf{I}$ to be all zeros, thus singular.

It was suspected that the use of the pseudo-inverse in \texttt{poLCA} was a way to handle singular matrices. It, however, does not solve the problem from the ground up as the covariance does need to be positive definite in the first place for it to make sense mathematically.

In order for $\mathbf{I}$ to be Cholesky decomposable, it must be positive-definite and, ideally, well-conditioned.

A positive-definite $\mathbf{I}$ can be achieved by preventing the posteriors $\theta_{ir}$ and estimated probabilities $\pi_{j,r,k}$ to take values of zero or one. Laplace smoothing may be appropriate which encourages $\theta_{ir}$ and $\pi_{j,r,k}$ to move closer to 0.5, rather than zero or one. This can be done by replacing the calculations of $\theta_{ir}$ and $\pi_{j,r,k}$ with
\begin{equation}
    \Tilde{\pi}_{j,r,k} = \dfrac{N\pi_{j,r,k} + 1}{N + K_j}
\end{equation}
and
\begin{equation}
    \Tilde{\theta}_{i,r} = \dfrac{N\theta_{i,r,k} + 1}{N + R}
    \ .
\end{equation}
This would alter the calculations but should guarantee $\mathbf{I}$ to be positive definite. It may also be applied to the prior for good measure
\begin{equation}
    \Tilde{p}_{r} = \dfrac{N p_{r} + 1}{N + R}
    \ .
\end{equation}

However, $\mathbf{I}$ may not be well-conditioned and should be pre-conditioned so that the values in $\mathbf{L}$ are sensible for the triangular solver. Let $\mathbf{D}$ be a diagonal matrix with elements $\sqrt{\text{diag}\left[\mathbf{I}\right]}$. Then the linear equation can be written as
\begin{equation}
    \Sigma = \mathbf{J}\T \mathbf{D} \left(\mathbf{D}\mathbf{I}\mathbf{D}\right)^{-1} \mathbf{D} \mathbf{J} \ .
\end{equation}
Instead, a Cholesky decomposition of $\mathbf{D}\mathbf{I}\mathbf{D} = \mathbf{L}\mathbf{L}\T$ can be taken, hoping it would have a better condition number than $\mathbf{I}$ by itself. Then the standard error are the column sum of squares of $\mathbf{L}^{-1} \left(\mathbf{D}\mathbf{J}\right)$.

\section{Further Reading}
\cite{doi:10.1080/01621459.1997.10473658} goes into the algebra of these calculations a bit further. \cite{10.2307/2345828} mentions on calculating the observed information matrix when using the EM algorithm. Lastly, the bootstrap \citep{efron1979bootstrap} is a numerical way of calculating the standard error by resampling from the data.

\bibliographystyle{apalike}
\bibliography{bib}

\end{document}
